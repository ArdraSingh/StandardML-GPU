# Final Writeup

## Overview
We developed StandardML-GPU, a Standard ML library and extension that allows a user to 
interface with CUDA, and allow Standard ML to take advantage of the computing power of the GPU. 
Our library provides an interface between raw CUDA code and Standard ML, an abstraction from
C/CUDA memory management and transfer, and a series of 
[`SEQUENCE`](http://www.cs.cmu.edu/~15210/docs/sig/sequence/SEQUENCE.html) data structre
implementations, which will be discussed in the report. Our library acts as a 
domain specific language within Standard ML, where a user
is able to express thier algorithm in terms of high level operations like `scan`, `tabulate`, 
and `reduce`, and is able to beat out **all other options** aside from handwritten CUDA. 

## Introduction
Sometimes, a few lines of code are worth a thousand words. Below we compare two 
implementations of parenthesis matching, The first using StandardML, and then with
our StandardMl-GPU library. The algorithem determines whether a sequence of 
parentheses ( where ( = 1 and ) = -1 ) is matched or not by computing a prefix sum
across the input sequence, checking to make sure element at the end is zero, and
the sum never drops below zero along the prefix sum. 

### Standard ML
~~~~ocaml
fun match_parens s = 
  let
    val (s', last) = Seq.scanIncl (op +) 0 s
  in
    last = 0 andalso (Seq.reduce Int.min 0 s') >= 0)
  end
~~~~
### Standard ML with GPU
~~~~ocaml
fun match_parens s = 
  let
    val (s', last) = GPUSeq.scanIncl Lambdas.add 0 s
  in
    last = 0 andalso (GPUSeq.reduce Lambdas.min 0 s') >= 0)
  end
~~~~

With almost no changes to the code, other than referencing our new GPUSeq structure and swapping 
out the higher order function with one from the library's Lambdas structure, this code will now
compile and run on the GPU. 

Thus, with little effort, you can expect to see large performance gains
from your code! This code greatly outperforms the sequential code generated by the MLton compiler.
However, this really isn't too fair of a test with just a single core!


In paralell to this project, research at CMU has been focoused on running Standard ML in parallel 
on multicore CPU's. The Mlton-Spoonhower compiler simillarly implements a sequence library.
How does our implementation fare against a well written, parallel `SEQUENCE` implementation for that research,
running on 72 cores? (These results should be taken with a grain of salt, as we had to run these tests
on a different machine than the one other tests were run on).


Perhaps even that comparison was not fair, there is an implicit amount of overhead in StandardML that must be maintained whcih can introduce overhead with large computations. To get a real taste for the acceleration that our library offers, we now compare our run-time speed to that of Thrust, where a computationaly comperable algorithem can be written independently. 

 

### Comparing performance of SML libraries 
<iframe width="640" height="540" frameborder="1" scrolling="no" src="https://plot.ly/~bhoughton/1.embed"></iframe>

As we can see, our performance is competitive with Thrust on smaller inputs, and beats out Thrust on larger 
input sizes, which is a much more fair evaluation of our library. 

Additionally, even at 72 cores, a single GTX 1080 handily beats out parallel Standard ML on this parentheses matching problem.
While alot of optimizations can be made by programmers using our library, the research code or thrust (such as
of writing sections of the primitives yourself, or compressing data to use less memory), in terms of 
performance given ease of use, our library is the clear choice. 

## Background
Functional programming naturally describes transformations of data in a declarative manner.
Since functional languages are extremely easy for programmers to express algorithmic ideas in, 
we hope that their code can run fast and efficiently without having to translate 
their code into another language. Additionally, they should be able to use the same functional 
programming methodology and see good performance without drastic changes to their own code in 
Standard ML. 

Functional programs expressed with [`SEQUENCE`](http://www.cs.cmu.edu/~15210/docs/sig/sequence/SEQUENCE.html) 
primitives allows for powerful abstractions for algorithm design, and leaves the dirty work of 
efficiently implementing these primitives up to the implementer of the library. Primitives like
`scan`, `reduce`, `filter`, etc. are extremely data parallel, and map well to the GPU platform. 
However, there previously was no way that functional programmers could use these ideas from a functional
setting, having to resort to using libraries like [Thrust](http://docs.nvidia.com/cuda/thrust/#axzz4gcJAv4tP) in 
C to get the same kind of abstraction. 

Allowing for an interface between Standard ML and the GPU has a number of difficulties, which 
relate to the restricted nature of GPU computation, in contrast to the lack of restrictions in 
terms of memory management and syntactic constructs of Standard ML. To be specific the difficulties lie in : 
1. Providing an intuitive abstraction for memory management for device memory, since Standard ML does not have manual memory management.
2. Providing a flexible interface (as much polymorphism, higher order functions as possible) that allows users to write mostly SML, and more functional style programs.
3. Implementing very efficient primitives that allow for arbitrary user-defined functions.
4. Work around the structure of parallel function programs to allow for more efficient use of hardware.

## Abstraction

## Implementation

### Memory Management

### Primitives

### Types and Tuples

Due to the nature of the Foreign Function Interface between Standard ML and C, the liberties
we are allowed to take with typing

### Fusing

Classically, programs that exhibit behavior like the following are usually bandwidth bound, and 
require manual fusing by the user.

~~~~ocaml
val s1 = Seq.map (fn x => 2 * x) S
val s2 = Seq.map (fn x => x + 1) s1
val result = Seq.reduce (op * ) 1 s2
~~~~~

This code could have both maps turned into just 1 map, so that data is accessed only once. In the general
case however, it is easier for users to think about thier code when they can break up the data accesses 
and operations on the structure. With this in mind,
our library supports a powerful feature that allows the user to fuse multiple sequence operations into a 
single kernel launch decreasing the overhead and dramatically improving performance. All mapping operations
are evaluated lazily, only applied when the `force()` function is called, or when a primitive that needs the 
mapped values, like `scan` or `reduce` is called. 

Below we have an example of our fusing in action. Our example code will call a series of maps, and then
a reduce on the mapped data. Notice how increasing the number of maps before reduction causes more
and more overhead for thrust and Standard ML, but the time taken by our library stays relatively constant. 

# graph here

## Performance and Analysis 

While our primitives beat Thrust, there are better libraries to compare implementations against, 
such as CUB. Additionally, we can see that while our performance on primitives was better, in a 
program that used a group of primitives in conjunction with each other, we werent able to beat
Thrust quite as handily, which means that we can definately work to pipeline and group operations
together better. 

## Conclusion

## Work Distribution

Both partners did equal work!

## Checkpoint
Find our checkpoint write-up [here](checkpoint.md).

## Proposal
Find our proposal write-up [here](proposal.md).
