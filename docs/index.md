# Final Writeup

## Overview
We developed StandardML-GPU, a Standard ML library and extension that allows a user to 
interface with CUDA, and allow Standard ML to take advantage of the computing power of the GPU. 
Our library provides an interface between raw CUDA code and Standard ML, an abstraction from
C/CUDA memory management and transfer, and a series of 
[`SEQUENCE`](http://www.cs.cmu.edu/~15210/docs/sig/sequence/SEQUENCE.html) data structre
implementations, which will be discussed in the report. Our library acts as a 
domain specific language within Standard ML, where a user
is able to express thier algorithm in terms of high level operations like `scan`, `tabulate`, 
and `reduce`, and is able to beat out **all other options** aside from handwritten CUDA. 

## Introduction
Sometimes, a few lines of code are worth a thousand words. Below we compare two 
implementations of parenthesis matching, The first using StandardML, and then with
our StandardMl-GPU library. The algorithem determines whether a sequence of 
parentheses ( where ( = 1 and ) = -1 ) is matched or not by computing a prefix sum
across the input sequence, checking to make sure element at the end is zero, and
the sum never drops below zero along the prefix sum. 

### Standard ML
~~~~ocaml
fun match_parens s = 
  let
    val (s', last) = Seq.scanIncl (op +) 0 s
  in
    last = 0 andalso (Seq.reduce Int.min 0 s') >= 0)
  end
~~~~
### Standard ML with GPU
~~~~ocaml
fun match_parens s = 
  let
    val (s', last) = GPUSeq.scanIncl Lambdas.add 0 s
  in
    last = 0 andalso (GPUSeq.reduce Lambdas.min 0 s') >= 0)
  end
~~~~

With almost no changes to the code, other than referencing our new GPUSeq structure and swapping 
out the higher order function with one from the library's Lambdas structure, this code will now
compile and run on the GPU. 

Thus, with little effort, you can expect to see large performance gains
from your code! This code greatly outperforms the sequential code generated by the MLton compiler.
However, this really isn't too fair of a test with just a single core!


In paralell to this project, research at CMU has been focoused on running Standard ML in parallel 
on multicore CPU's. The Mlton-Spoonhower compiler simillarly implements a sequence library.
How does our implementation fare against a well written, parallel `SEQUENCE` implementation for that research,
running on 72 cores? (These results should be taken with a grain of salt, as we had to run these tests
on a different machine than the one other tests were run on).


Perhaps even that comparison was not fair, there is an implicit amount of overhead in StandardML that must be maintained whcih can introduce overhead with large computations. To get a real taste for the acceleration that our library offers, we now compare our run-time speed to that of Thrust, where a computationaly comperable algorithem can be written independently. 

 

### Comparing performance of SML libraries 
<iframe width="640" height="540" frameborder="1" scrolling="no" src="https://plot.ly/~bhoughton/1.embed"></iframe>

As we can see, our performance is competitive with Thrust on smaller inputs, and beats out Thrust on larger 
input sizes, which is a much more fair evaluation of our library. 

Additionally, even at 72 cores, a single GTX 1080 handily beats out parallel Standard ML on this parentheses matching problem.
While there are a lot of low level optimizations that can be made, such as
of writing sections of the primitives in raw CUDA, or compressing data to use less memory, in terms of 
performance given ease of use, our library is the clear choice. 

## Background
Functional programming naturally describes transformations of data in a declarative manner.
Since functional languages are extremely easy for programmers to express algorithmic ideas in, 
we hope that their code can run fast and efficiently without having to translate 
their code into another language. Additionally, they should be able to use the same functional 
programming methodology and see good performance without drastic changes to their own code in 
Standard ML. 

Functional programs expressed with [`SEQUENCE`](http://www.cs.cmu.edu/~15210/docs/sig/sequence/SEQUENCE.html) 
primitives allows for powerful abstractions for algorithm design, and leaves the dirty work of 
efficiently implementing these primitives up to the implementer of the library. Primitives like
`scan`, `reduce`, `filter`, etc. are extremely data parallel, and map well to the GPU platform. 
However, there previously was no way that functional programmers could use these ideas from a functional
setting, having to resort to using libraries like [Thrust](http://docs.nvidia.com/cuda/thrust/#axzz4gcJAv4tP) in 
C to get the same kind of abstraction. 

Allowing for an interface between Standard ML and the GPU has a number of difficulties, which 
relate to the restricted nature of GPU computation, in contrast to the lack of restrictions in 
terms of memory management and syntactic constructs of Standard ML. To be specific the difficulties lie in : 
1. Providing an intuitive abstraction for memory management for device memory, since Standard ML does not have manual memory management.
2. Providing a flexible interface (as much polymorphism and higher order functions as possible) that allows users to write mostly SML, and more functional style programs.
3. Implementing very efficient primitives that allow for arbitrary user-defined functions.
4. Work around the structure of parallel function programs to allow for more efficient use of hardware.

## Abstraction
Throughout our library users do not have be familiar with any interface to the GPU. By extending the Standard ML library
through the Foreign Function Interface, users are able to exploit the builtin function lambdas and
SML GPU sequence functions in order to accelerate their workload. In the case that a user is familiar with 
CUDA or would like to bring an expert in to optimize an important calculation, our interface is easily extendable to 
allow this.

### GPU Arrays
In the structure `GPUArray`, we define an abstraction for arrays that are hosted on the device. Using this interface, 
through methods like `initInt` or `copyIntoIntArray`, a user can move between these objects and the built in
Standard ML [`Array`](http://sml-family.org/Basis/array.html) structure to abstract away grungy manual copying, 
as well as ensuring that device data is never accessed from the host. A more experienced user can directly
maniupate the data that these arrays point to, and explicitly launch their own more specified kernels if needed. 

### GPU Sequences
Our GPU Sequences are built on top of these GPU Arrays and are intended to be the main point of usage for the library. 
We support a smaller subset of the operations supported by the full 
[15-210 `SEQUENCE` Library](http://www.cs.cmu.edu/~15210/docs/sig/sequence/SEQUENCE.html). Through limitations of 
the Foreign Function Interface the Standard ML provides, we must restrict the types of these sequences from purely
polymorphic to integers, floats, and integer tuples. 

## Implementation

### Memory Management
The GPU Array structure has interface calls to CUDA, where we allocate arrays directly on the GPU,
and pass around pointers (not revealed to the user) to these arrays on the Standard ML side. 
Using other functions in the GPU Array structure we handle copying from and to local arrays on the 
Standard ML side. We used the type and module system to make memory management feel just like
using a standard module, where the only ways to create and manipulate objects is through the 
interface that the module provides. The key difficulty in this portion of the development was
figuring out the intricacies of the Standard ML Foreign Function Interface, and making sure the 
Standard ML actually interfaced with CUDA without errors. 

### Primitives

#### Map, Tabulate and Zip
`map`, `tabulate`, and `zip` are all extremely data parallel operations, and their GPU implementations
were as expected. The key insight that we made for these operations, as well as the rest of the 
primitives was the lack of necessity to maintain persistence of data throughout execution. Most
of the time, a user does not need to make a new copy of a 1 billion element array every time
it is modified, which must be done in a purely functional runtime. So instead, we decided to 
have our library be the opposite : we will always perform mutation directly on the sequence given
to avoid unecessary copying. If a user does not want their data dirctly modified, then they can 
use our interface to make a copy of thier data. To attain good performance, we sadly must
do away with persistence, since a large portion of these primitives are bandwidth bound. 
Any additional memory operations will just slow the performance down even more. 

#### Reduce
Our efficient implementation of reduce was inspired by this 
[Nvidia blog post](https://devblogs.nvidia.com/parallelforall/faster-parallel-reductions-kepler/).
The key to our fast implementation of reduce is the `__shfl_down` intrinsic. `__shfl_down` allows
for extremely fast in-warp communication - **much faster than shared memory**. `__shfl_down` 
allows us to replace large shared memory reads and writes with a single instruction that allows
"threads" in a warp to communicate with each other. For example, if we were to execute this code 
below, we would see the following behavior in our warp. 
~~~~c
int i = threadIdx.x % 32;
int j = __shfl_down(i, 2, 8);
~~~~~
![warp image](https://devblogs.nvidia.com/parallelforall/wp-content/uploads/2014/02/shfl_down.png)

We implement a warp-local reduction by shifting down and combining results by a power of 2
each iteration. At the first level, everything is shifted down 1 row, then 2, then 4 and so on.
On the last iteration, the final reduced value for that warp is held by the "thread" at warp index 0. 

This instrisic leads us nicely to a reduce implementation. Our algorithm does the following : 
1. First, we allocate an array for partial results whose size is equal to the number of blocks we split our input into.
2. For each block, we have each warp compute a warp-local reduction, and store this in a shared array.
3. Then we have the first warp do another warp-local reduction over each warp's stored results. 
4. Lastly, we launch 1 more kernel, to do the same procedure on each block's partial reduction. 

This reduction algorithm performs extremely well, as we can see here : 

# brandon insert reduce graph here

The key to this performance is the `__shfl_down` intrinsic. We are able to perform a warp-local 
reduction in basically 5 steps, as opposed to large amounts of reading and writing into shared memory. 

#### Scan

#### Filter

### Arbitrary Functions

### Types and Tuples

Due to the nature of the Foreign Function Interface between Standard ML and C, the liberties
we are allowed to take with typing

### Fusing

Classically, programs that exhibit behavior like the following are usually bandwidth bound, and 
require manual fusing by the user.

~~~~ocaml
val s1 = Seq.map (fn x => 2 * x) S
val s2 = Seq.map (fn x => x + 1) s1
val result = Seq.reduce (op * ) 1 s2
~~~~~

This code could have both maps turned into just 1 map, so that data is accessed only once. In the general
case however, it is easier for users to think about thier code when they can break up the data accesses 
and operations on the structure. With this in mind,
our library supports a powerful feature that allows the user to fuse multiple sequence operations into a 
single kernel launch decreasing the overhead and dramatically improving performance. All mapping operations
are evaluated lazily, only applied when the `force()` function is called, or when a primitive that needs the 
mapped values, like `scan` or `reduce` is called. 

Below we have an example of our fusing in action. Our example code will call a series of maps, and then
a reduce on the mapped data. Notice how increasing the number of maps before reduction causes more
and more overhead for thrust and Standard ML, but the time taken by our library stays relatively constant. 

# graph here

We implemented fusing by adjusting how sequences were represented for the structure
`FusedINTGPUSequence`. Now, these structures also carried around a list of functions
that needed to be applied. Whenever a mapping operation was called, a kernel was not 
launched. Instead, this function was just pushed onto the list. During any aggregate 
operation, like a `scan` or a `reduce`, before the data was operated on, all the 
functions in the list were immediately applied to the data inside the kernel that was
doing the reduction or the scan. This idea can be extended further in a functional programming
setting, where we can represent sequences with functions, and delay computation as long as possible. 

## Performance and Analysis 

While our primitives beat Thrust, there are better libraries to compare implementations against, 
such as CUB. Additionally, we can see that while our performance on primitives was better, in a 
program that used a group of primitives in conjunction with each other, we werent able to beat
Thrust quite as handily, which means that we can definately work to pipeline and group operations
together better. 

## Conclusion

## Work Distribution

Both partners did equal work!

## Checkpoint
Find our checkpoint write-up [here](checkpoint.md).

## Proposal
Find our proposal write-up [here](proposal.md).
