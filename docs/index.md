# Final Writeup

## Overview
We developed StandardML-GPU, a Standard ML library and extension that allows a user to 
interface with CUDA, and allow Standard ML to take advantage of the computing power of the GPU. 
Our library provides an interface between raw CUDA code and Standard ML, an abstraction from
C/CUDA memory management and transfer, and a series of 
[`SEQUENCE`](http://www.cs.cmu.edu/~15210/docs/sig/sequence/SEQUENCE.html) data structre
implementations, which will be discussed in the report. Our library acts as a 
domain specific language within Standard ML, where a user
is able to express thier algorithm in terms of high level operations like `scan`, `tabulate`, 
and `reduce`, and is able beat out **all other options** aside from handwritten CUDA. 

## Introduction
Sometimes, a few lines of code are worth a thousand words. Here is an example of how to 
tell whether a sequence of parentheses ( where ( = 1 and ) = -1 ) is matched or not,
using Standard ML, and with our GPU library. The algorithm computes a prefix sum
across the input sequence, and then checks to make sure element at the end is a 0, and
the sum never drops below zero along the prefix sum. 

### Standard ML
~~~~ocaml
fun match_parens s = 
  let
    val (s', last) = Seq.scanIncl (op +) 0 s
  in
    last = 0 andalso (Seq.reduce Int.min 0 s') >= 0)
  end
~~~~
### Standard ML with GPU
~~~~ocaml
fun match_parens s = 
  let
    val (s', last) = GPUSeq.scanIncl Lambdas.add 0 s
  in
    last = 0 andalso (GPUSeq.reduce Lambdas.min 0 s') >= 0)
  end
~~~~

With almost no changes to the code, other than referencing a GPUSeq structure as opposed
to Seq, and swapping out the higher order functions with references to functions inside
a Lambdas structure, this code will now compile and run on the GPU. 

With just a few changes to your code, you can expect to see large performance gains
from your code! This code will greatly outperform the sequential code generated by the MLton compiler.
However, this really isn't too fair of a test. To get a real taste for the acceleration that 
our library offers, compare our run-time speed to that of Thrust, where a similiar style of
code can be written, albeit in C. 


Lastly, research at CMU has been going on to run Standard ML in parallel on multicore CPU's. How does
our implementation fare against a well written, parallel `SEQUENCE` implementation for that research,
running on 72 cores? (These results should be taken with a grain of salt, as we had to run these tests
on a different machine than the one other tests were run on). 

### Comparing performance of SML libraries 
<iframe width="640" height="540" frameborder="0" scrolling="no" src="https://plot.ly/~bhoughton/1.embed"></iframe>

As we can see, our performance is equal to Thrust on smaller inputs, and beats out Thrust on larger 
input sizes, which is a much more fair evaluation of our library. 

Additionally, even at 72 cores, 1 GTX 1080 handily beats out parallel Standard ML on this parentheses matching problem.
While alot of optimizations can be made by programmers using our library, the research code or thrust (such as
of writing sections of the primitives yourself, or compressing data to use less memory), in terms of 
performance given ease of use, our library is the clear choice. 

## Background
Functional programming naturally describes transformations of data in a declarative manner.
Since functional languages are extremely easy for programmers to express algorithmic ideas in, 
we hope that thier code can run fast and efficiently without having to translate 
thier code into another language Additionally, they should be able to use the same functional 
programming methodology and see good performance without drastic changes to thier own code in 
Standard ML. 

Functional programs expressed with [`SEQUENCE`](http://www.cs.cmu.edu/~15210/docs/sig/sequence/SEQUENCE.html) 
primitives allow for powerful abstractions for algorithm design, and leaves the dirty work of 
efficiently implementing these primitives up to the implementer of the library. Primitives like
`scan`, `reduce`, `filter`, etc. are extremely data parallel, and map well to the GPU platform. 
However, there previously was no way that functional programmers could use these ideas from a functional
setting, having to resort to using libraries like [Thrust](http://docs.nvidia.com/cuda/thrust/#axzz4gcJAv4tP) in 
C to get the same kind of abstraction. 

Allowing for an interface between Standard ML and the GPU has a number of difficulties, which 
relate to the restricted nature of GPU computation, in contrast to the lack of restrictions in 
terms of memory management and syntactic constructs of Standard ML. To be specific the difficulties lie in : 
1. Providing an intuitive abstraction for memory management for device memory, since Standard ML does not have manual memory management.
2. Providing a flexible interface (as much polymorphism, higher order functions as possible) that allows users to write mostly SML, and more functional style programs.
3. Implementing very efficient primitives that allow for arbitrary user-defined functions.
4. Work around the structure of parallel function programs to allow for more efficient use of hardware.

## Abstraction

## Implementation

### Memory Management

### Primitives

### Types and Tuples

### Fusing

Classically, programs that exhibit behavior like the following are usually bandwidth bound, and 
require manual fusing by the user.

~~~~ocaml
val s1 = Seq.map (fn x => 2 * x) S
val s2 = Seq.map (fn x => x + 1) s1
~~~~~

This code could have both maps turned into just 1 map, so that data is accessed only once. However,
our library supports a powerful feature that allows the user to fuse multiple sequence operations into a 
single kernel launch decreasing the overhead and dramatically improving performance. All mapping operations
are evaluated lazily, only applied when the `force()` function is called, or when a primitive that needs the 
mapped values, like `scan` or `reduce` is called. 

Below we have an example of our fusing in action. Our example code will call a series of maps, and then
a reduce on the mapped data. Notice how increasing the number of maps before reduction causes more
and more overhead for thrust and Standard ML, but the time taken by our library stays relatively constant. 

# graph here

## Performance and Analysis 

### Baseline Primitive Performance

### Fused Primitive Performance

## Conclusion

## Work Distribution

Both partners did equal work!

## Checkpoint
Find our checkpoint write-up [here](checkpoint.md).

## Proposal
Find our proposal write-up [here](proposal.md).
